{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Classification\n",
    "1. Import Data\n",
    "2. Encode binary and categorical data\n",
    "3. select features based on previous analysis\n",
    "4. Try RandomForest and XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import cm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_processing import (create_contingency_table,\n",
    "                                calculate_conditional_prob_bin,\n",
    "                                encode_my_categorical_labels,\n",
    "                                calculate_conditional_prob_cat,\n",
    "                                estimate_cond_prob_density,\n",
    "                                bin_myFeature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# total of features:       57\n",
      "# of binary features:      17\n",
      "# of categorical features: 14\n",
      "# of other features:       26\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv', header=0)\n",
    "\n",
    "all_fs = train.columns[2:]\n",
    "binary_fs = sorted([f for f in all_fs if '_bin' in f])\n",
    "categorical_fs = sorted([f for f in all_fs if '_cat' in f])\n",
    "other_fs = sorted([f for f in all_fs\n",
    "            if f not in binary_fs\n",
    "            if f not in categorical_fs])\n",
    "\n",
    "print(\"# total of features: %8d\" % len(all_fs))\n",
    "print(\"# of binary features: %7d\" % len(binary_fs))\n",
    "print(\"# of categorical features: %1d\" % len(categorical_fs))\n",
    "print(\"# of other features: %8d\" % len(other_fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Feature Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals_encoded = []\n",
    "for i, fs in enumerate(categorical_fs):\n",
    "    categoricals_encoded.append(encode_my_categorical_labels(train[fs]))\n",
    "categoricals_encoded = pd.concat(categoricals_encoded, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ps_car_01_cat_NaN', 'ps_car_01_cat_0', 'ps_car_01_cat_1',\n",
       "       'ps_car_01_cat_2', 'ps_car_01_cat_3', 'ps_car_01_cat_4',\n",
       "       'ps_car_01_cat_5', 'ps_car_01_cat_6', 'ps_car_01_cat_7',\n",
       "       'ps_car_01_cat_8',\n",
       "       ...\n",
       "       'ps_ind_04_cat_0', 'ps_ind_04_cat_1', 'ps_ind_05_cat_NaN',\n",
       "       'ps_ind_05_cat_0', 'ps_ind_05_cat_1', 'ps_ind_05_cat_2',\n",
       "       'ps_ind_05_cat_3', 'ps_ind_05_cat_4', 'ps_ind_05_cat_5',\n",
       "       'ps_ind_05_cat_6'],\n",
       "      dtype='object', length=184)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categoricals_encoded.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define select features based on nb02-nb03 analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists of feature labels\n",
    "select_binary_fs = ['ps_ind_07_bin', 'ps_ind_10_bin', 'ps_ind_12_bin', 'ps_ind_13_bin', 'ps_ind_17_bin']\n",
    "\n",
    "select_categorical_fs1 = ['ps_car_04_cat_5', 'ps_car_04_cat_7', 'ps_car_06_cat_2', 'ps_car_06_cat_5', \n",
    "                          'ps_car_06_cat_8', 'ps_car_11_cat_18', 'ps_car_11_cat_41', 'ps_ind_05_cat_2'] \n",
    "\n",
    "select_categorical_fs2 = ['ps_car_04_cat_5', 'ps_car_04_cat_6', 'ps_car_04_cat_7', 'ps_car_04_cat_9', \n",
    "                          'ps_car_06_cat_17', 'ps_car_06_cat_2', 'ps_car_06_cat_5', 'ps_car_06_cat_8', \n",
    "                          'ps_car_09_cat_1', 'ps_car_11_cat_18', 'ps_car_11_cat_21', 'ps_car_11_cat_4', \n",
    "                          'ps_car_11_cat_41', 'ps_car_11_cat_58', 'ps_car_11_cat_63', 'ps_car_11_cat_75', \n",
    "                          'ps_car_11_cat_93', 'ps_car_11_cat_97', 'ps_ind_05_cat_2', 'ps_ind_05_cat_6'] \n",
    "\n",
    "select_categorical_fs3 = ['ps_car_01_cat_9', 'ps_car_02_cat_0', 'ps_car_03_cat_1', 'ps_car_04_cat_3', \n",
    "                          'ps_car_04_cat_5', 'ps_car_04_cat_6', 'ps_car_04_cat_7', 'ps_car_04_cat_9', \n",
    "                          'ps_car_06_cat_13', 'ps_car_06_cat_15', 'ps_car_06_cat_17', 'ps_car_06_cat_2', \n",
    "                          'ps_car_06_cat_5', 'ps_car_06_cat_8', 'ps_car_06_cat_9', 'ps_car_07_cat_0', \n",
    "                          'ps_car_09_cat_1', 'ps_car_09_cat_4', 'ps_car_11_cat_100', 'ps_car_11_cat_18', \n",
    "                          'ps_car_11_cat_21', 'ps_car_11_cat_3', 'ps_car_11_cat_33', 'ps_car_11_cat_4', \n",
    "                          'ps_car_11_cat_41', 'ps_car_11_cat_55', 'ps_car_11_cat_56', 'ps_car_11_cat_58', \n",
    "                          'ps_car_11_cat_61', 'ps_car_11_cat_63', 'ps_car_11_cat_69', 'ps_car_11_cat_71', \n",
    "                          'ps_car_11_cat_72', 'ps_car_11_cat_75', 'ps_car_11_cat_93', 'ps_car_11_cat_97', \n",
    "                          'ps_ind_05_cat_2', 'ps_ind_05_cat_4', 'ps_ind_05_cat_6']\n",
    "\n",
    "select_categorical_fs4 = ['ps_car_01_cat_0', 'ps_car_01_cat_1', 'ps_car_01_cat_11', 'ps_car_01_cat_9', \n",
    "                          'ps_car_02_cat_0', 'ps_car_03_cat_1', 'ps_car_04_cat_1', 'ps_car_04_cat_2', \n",
    "                          'ps_car_04_cat_3', 'ps_car_04_cat_5', 'ps_car_04_cat_6', 'ps_car_04_cat_7', \n",
    "                          'ps_car_04_cat_8', 'ps_car_04_cat_9', 'ps_car_06_cat_10', 'ps_car_06_cat_12', \n",
    "                          'ps_car_06_cat_13', 'ps_car_06_cat_15', 'ps_car_06_cat_16', 'ps_car_06_cat_17', \n",
    "                          'ps_car_06_cat_2', 'ps_car_06_cat_5', 'ps_car_06_cat_8', 'ps_car_06_cat_9', \n",
    "                          'ps_car_07_cat_0', 'ps_car_08_cat_0', 'ps_car_09_cat_1', 'ps_car_09_cat_4', \n",
    "                          'ps_car_11_cat_100', 'ps_car_11_cat_104', 'ps_car_11_cat_13', 'ps_car_11_cat_17', \n",
    "                          'ps_car_11_cat_18', 'ps_car_11_cat_20', 'ps_car_11_cat_21', 'ps_car_11_cat_3', \n",
    "                          'ps_car_11_cat_33', 'ps_car_11_cat_4', 'ps_car_11_cat_41', 'ps_car_11_cat_45', \n",
    "                          'ps_car_11_cat_50', 'ps_car_11_cat_55', 'ps_car_11_cat_56', 'ps_car_11_cat_58', \n",
    "                          'ps_car_11_cat_61', 'ps_car_11_cat_63', 'ps_car_11_cat_69', 'ps_car_11_cat_71', \n",
    "                          'ps_car_11_cat_72', 'ps_car_11_cat_75', 'ps_car_11_cat_79', 'ps_car_11_cat_89', \n",
    "                          'ps_car_11_cat_90', 'ps_car_11_cat_93', 'ps_car_11_cat_94', 'ps_car_11_cat_97', \n",
    "                          'ps_ind_05_cat_1', 'ps_ind_05_cat_2', 'ps_ind_05_cat_4', 'ps_ind_05_cat_5', \n",
    "                          'ps_ind_05_cat_6']\n",
    "\n",
    "select_categorical_neg_fs = ['ps_car_02_cat_1', 'ps_car_04_cat_0', 'ps_car_04_cat_4', 'ps_car_07_cat_1', \n",
    "                          'ps_car_11_cat_19', 'ps_car_11_cat_32', 'ps_car_11_cat_39', 'ps_car_11_cat_43', \n",
    "                          'ps_car_11_cat_57', 'ps_car_11_cat_7', 'ps_car_11_cat_99', 'ps_ind_05_cat_0']\n",
    "\n",
    "select_other_fs = ['ps_calc_05', 'ps_calc_07', 'ps_calc_13', 'ps_calc_14',\n",
    "                   'ps_car_12', 'ps_car_13', 'ps_ind_03', 'ps_ind_14', 'ps_reg_02', 'ps_reg_03']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames based on the above lists\n",
    "select_binaries = train[select_binary_fs]\n",
    "select_others = train[select_other_fs]\n",
    "\n",
    "select_cats1 = categoricals_encoded[select_categorical_fs1]\n",
    "select_cats2 = categoricals_encoded[select_categorical_fs2]\n",
    "select_cats3 = categoricals_encoded[select_categorical_fs3]\n",
    "select_cats4 = categoricals_encoded[select_categorical_fs4]\n",
    "select_cats_neg = categoricals_encoded[select_categorical_neg_fs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined the above DataFrames to create different sets of features\n",
    "select_feature1 = pd.concat([select_others, select_binaries, select_cats1, train.target], axis=1)\n",
    "select_feature2 = pd.concat([select_others, select_binaries, select_cats2, train.target], axis=1)\n",
    "select_feature3 = pd.concat([select_others, select_binaries, select_cats3, train.target], axis=1)\n",
    "select_feature4 = pd.concat([select_others, select_binaries, select_cats4, train.target], axis=1)\n",
    "select_feature5 = pd.concat([select_others, select_binaries, select_cats1, select_cats_neg, train.target], axis=1)\n",
    "select_feature6 = pd.concat([select_others, select_binaries, select_cats2, select_cats_neg, train.target], axis=1)\n",
    "select_feature7 = pd.concat([select_others, select_binaries, select_cats3, select_cats_neg, train.target], axis=1)\n",
    "select_feature8 = pd.concat([select_others, select_binaries, select_cats4, select_cats_neg, train.target], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selection1 feature space: (595212, 24)\n",
      "selection2 feature space: (595212, 36)\n",
      "selection3 feature space: (595212, 55)\n",
      "selection4 feature space: (595212, 77)\n",
      "selection5 feature space: (595212, 36)\n",
      "selection6 feature space: (595212, 48)\n",
      "selection7 feature space: (595212, 67)\n",
      "selection8 feature space: (595212, 89)\n"
     ]
    }
   ],
   "source": [
    "# print the sizes of the feature space\n",
    "print('selection1 feature space:', select_feature1.shape)\n",
    "print('selection2 feature space:', select_feature2.shape)\n",
    "print('selection3 feature space:', select_feature3.shape)\n",
    "print('selection4 feature space:', select_feature4.shape)\n",
    "print('selection5 feature space:', select_feature5.shape)\n",
    "print('selection6 feature space:', select_feature6.shape)\n",
    "print('selection7 feature space:', select_feature7.shape)\n",
    "print('selection8 feature space:', select_feature8.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selection1 feature space: (487439, 24)\n",
      "selection2 feature space: (487439, 36)\n",
      "selection3 feature space: (487439, 55)\n",
      "selection4 feature space: (487439, 77)\n",
      "selection5 feature space: (487439, 36)\n",
      "selection6 feature space: (487439, 48)\n",
      "selection7 feature space: (487439, 67)\n",
      "selection8 feature space: (487439, 89)\n"
     ]
    }
   ],
   "source": [
    "# Remove missing data entries\n",
    "selection1 = select_feature1.replace({-1:np.NaN}).dropna()\n",
    "selection2 = select_feature2.replace({-1:np.NaN}).dropna()\n",
    "selection3 = select_feature3.replace({-1:np.NaN}).dropna()\n",
    "selection4 = select_feature4.replace({-1:np.NaN}).dropna()\n",
    "selection5 = select_feature5.replace({-1:np.NaN}).dropna()\n",
    "selection6 = select_feature6.replace({-1:np.NaN}).dropna()\n",
    "selection7 = select_feature7.replace({-1:np.NaN}).dropna()\n",
    "selection8 = select_feature8.replace({-1:np.NaN}).dropna()\n",
    "\n",
    "# print the sizes of the feature space\n",
    "print('selection1 feature space:', selection1.shape)\n",
    "print('selection2 feature space:', selection2.shape)\n",
    "print('selection3 feature space:', selection3.shape)\n",
    "print('selection4 feature space:', selection4.shape)\n",
    "print('selection5 feature space:', selection5.shape)\n",
    "print('selection6 feature space:', selection6.shape)\n",
    "print('selection7 feature space:', selection7.shape)\n",
    "print('selection8 feature space:', selection8.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: class0/class1 = 96.36%/3.64%\n",
      "After : class0/class1 = 96.18%/3.82%\n"
     ]
    }
   ],
   "source": [
    "# Assess if frequency of class1 changed before and after pruning\n",
    "pre_total = len(train.target)\n",
    "pre_class0 = len(train.target[train.target == 0])\n",
    "pre_class1 = pre_total - pre_class0\n",
    "\n",
    "post_total = len(selection1)\n",
    "post_class0 = len(selection1.target[selection1.target == 0])\n",
    "post_class1 = post_total - post_class0\n",
    "\n",
    "print('Before: class0/class1 = %4.2f%%/%4.2f%%' % (100*pre_class0/pre_total, 100*pre_class1/pre_total))\n",
    "print('After : class0/class1 = %4.2f%%/%4.2f%%' % (100*post_class0/post_total, 100*post_class1/post_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's unlikely that the pruning of entries with NaN (or -1) entries affect the distribution of classes. *If any problemt arises later, make sure the pruning was not biased against class1 (there are many class0 entries, so bias against class0 shouldn't be a big issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "selections = [selection1, selection2, selection3, selection4, \n",
    "              selection5, selection6, selection7, selection8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryohayama/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== selection 1 ========\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9618    1.0000    0.9805     93762\n",
      "          1     0.0000    0.0000    0.0000      3726\n",
      "\n",
      "avg / total     0.9250    0.9618    0.9430     97488\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryohayama/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== selection 2 ========\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9618    1.0000    0.9805     93762\n",
      "          1     0.0000    0.0000    0.0000      3726\n",
      "\n",
      "avg / total     0.9250    0.9618    0.9430     97488\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryohayama/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== selection 3 ========\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9618    1.0000    0.9805     93762\n",
      "          1     0.0000    0.0000    0.0000      3726\n",
      "\n",
      "avg / total     0.9250    0.9618    0.9430     97488\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryohayama/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== selection 4 ========\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9618    1.0000    0.9805     93762\n",
      "          1     0.0000    0.0000    0.0000      3726\n",
      "\n",
      "avg / total     0.9250    0.9618    0.9430     97488\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryohayama/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== selection 5 ========\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9618    1.0000    0.9805     93762\n",
      "          1     0.0000    0.0000    0.0000      3726\n",
      "\n",
      "avg / total     0.9250    0.9618    0.9430     97488\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryohayama/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== selection 6 ========\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9618    1.0000    0.9805     93762\n",
      "          1     0.0000    0.0000    0.0000      3726\n",
      "\n",
      "avg / total     0.9250    0.9618    0.9430     97488\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryohayama/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== selection 7 ========\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9618    1.0000    0.9805     93762\n",
      "          1     0.0000    0.0000    0.0000      3726\n",
      "\n",
      "avg / total     0.9250    0.9618    0.9430     97488\n",
      "\n",
      "======== selection 8 ========\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9618    1.0000    0.9805     93762\n",
      "          1     0.0000    0.0000    0.0000      3726\n",
      "\n",
      "avg / total     0.9250    0.9618    0.9430     97488\n",
      "\n",
      "CPU times: user 9min 32s, sys: 1.62 s, total: 9min 33s\n",
      "Wall time: 1min 21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryohayama/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rstate = 55\n",
    "reports = []\n",
    "for i in range(len(selections)):\n",
    "    selection = selections[i]\n",
    "    X = selection.iloc[:, :-1]\n",
    "    y = selection.iloc[:, -1]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rstate)\n",
    "    rfc = RandomForestClassifier(max_depth=10, n_estimators=100, n_jobs=8)\n",
    "    rfc.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = rfc.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred, digits=4,\n",
    "                                   labels=None, target_names=None)\n",
    "    print('======== selection %d ========' % (i+1))\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boost Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== selection 1 ========\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9618    1.0000    0.9805     93762\n",
      "          1     1.0000    0.0003    0.0005      3726\n",
      "\n",
      "avg / total     0.9633    0.9618    0.9431     97488\n",
      "\n",
      "======== selection 2 ========\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9618    1.0000    0.9805     93762\n",
      "          1     0.0000    0.0000    0.0000      3726\n",
      "\n",
      "avg / total     0.9250    0.9618    0.9430     97488\n",
      "\n",
      "======== selection 3 ========\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9618    1.0000    0.9805     93762\n",
      "          1     0.5000    0.0003    0.0005      3726\n",
      "\n",
      "avg / total     0.9441    0.9618    0.9431     97488\n",
      "\n",
      "======== selection 4 ========\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9618    1.0000    0.9805     93762\n",
      "          1     0.5000    0.0005    0.0011      3726\n",
      "\n",
      "avg / total     0.9441    0.9618    0.9431     97488\n",
      "\n",
      "======== selection 5 ========\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9618    1.0000    0.9805     93762\n",
      "          1     0.0000    0.0000    0.0000      3726\n",
      "\n",
      "avg / total     0.9250    0.9617    0.9430     97488\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryohayama/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== selection 6 ========\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9618    1.0000    0.9805     93762\n",
      "          1     0.0000    0.0000    0.0000      3726\n",
      "\n",
      "avg / total     0.9250    0.9618    0.9430     97488\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryohayama/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== selection 7 ========\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9618    1.0000    0.9805     93762\n",
      "          1     0.0000    0.0000    0.0000      3726\n",
      "\n",
      "avg / total     0.9250    0.9618    0.9430     97488\n",
      "\n",
      "======== selection 8 ========\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9618    1.0000    0.9805     93762\n",
      "          1     0.0000    0.0000    0.0000      3726\n",
      "\n",
      "avg / total     0.9250    0.9618    0.9430     97488\n",
      "\n",
      "CPU times: user 24min, sys: 2.71 s, total: 24min 3s\n",
      "Wall time: 3min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rstate = 55\n",
    "reports = []\n",
    "for i in range(len(selections)):\n",
    "    selection = selections[i]\n",
    "    X = selection.iloc[:, :-1]\n",
    "    y = selection.iloc[:, -1]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rstate)\n",
    "    clf = XGBClassifier(max_depth=10, n_estimators=100, n_jobs=8)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred, digits=4,\n",
    "                                   labels=None, target_names=None)\n",
    "    print('======== selection %d ========' % (i+1))\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Clearly both classifiers perform pretty poorly with the huge imbalance in the class distribution (XGBoost marginally better?). Try SMOTE-ing class1 and RUS-ing (possibly with Tomek links) separately or simultaneously to raise the fraction of class1 during training. Testing should be done with imbalanced set. Another approach would be to go for anomally detection instead of classification but I'd need to learn how to do so first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
